{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7783d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b675b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "references_links = pd.read_csv(r'../src/data/raw/references/references_links.csv',\n",
    "                               sep=';')\n",
    "extracted_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80810ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetData:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def from_reddit(self, url: str, qtd_chamadas: int=5) -> list[dict]:\n",
    "        json_result_list = []\n",
    "        tentativa = qtd_chamadas\n",
    "        aux = 0\n",
    "\n",
    "        # Primeira requisição\n",
    "        response = requests.get(url)\n",
    "        json_object = response.json()\n",
    "\n",
    "        next_page = json_object['data']['after']\n",
    "        data_exists = json_object['data']\n",
    "\n",
    "        for item in json_object['data']['children']:\n",
    "            aux = aux+1\n",
    "        for page in range(aux):\n",
    "            json_result_list.append(json_object['data']['children'][page]['data']['selftext'])\n",
    "\n",
    "        # Loop para paginação\n",
    "        while data_exists:\n",
    "            if tentativa > 0:\n",
    "                time.sleep(20)\n",
    "                print(\"Next page:\", next_page)\n",
    "                response = requests.get(url, params={'after': next_page})\n",
    "                json_object = response.json()\n",
    "\n",
    "                # Pega a chave selftext de cada post, por página\n",
    "                aux = 0\n",
    "                for item in json_object['data']['children']:\n",
    "                    aux = aux+1\n",
    "                for page in range(aux):\n",
    "                    json_result_list.append(json_object['data']['children'][page]['data']['selftext'])\n",
    "\n",
    "                next_page = json_object['data'].get('after')\n",
    "                data_exists = json_object['data']\n",
    "                tentativa = tentativa - 1\n",
    "            else: break\n",
    "        return json_result_list\n",
    "          \n",
    "def from_website(self, references_links: pd.DataFrame) -> pd.DataFrame:\n",
    "    options = Options()\n",
    "    options.add_argument('--headless=new')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--log-level=3')\n",
    "    extracted_text = []\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    for _, row in references_links.iterrows():\n",
    "        journal = row['journal']\n",
    "        link = row['link']\n",
    "        try:\n",
    "            driver.get(link)\n",
    "            time.sleep(5)\n",
    "\n",
    "            if journal == 'einvestidor':\n",
    "                xpath = '/html/body/div[2]/article/div[3]'\n",
    "            elif journal == 'cnnbrasil':\n",
    "                xpath = '/html/body/div[2]/article/div[3]'\n",
    "            elif journal == 'valorinveste':\n",
    "                xpath = '/html/body/div[1]/main/div[2]/div/article/div[1]/div[3]'\n",
    "            elif journal == 'agenciabrasil':\n",
    "                xpath = '/html/body/main/div[2]/div/div/div[1]/div/div[5]'\n",
    "            elif journal == 'estadao':\n",
    "                xpath = '/html/body/div[1]/main/article'\n",
    "            elif journal == 'f5folha':\n",
    "                xpath = '/html/body/div[9]/div/div[1]/article/div[3]'\n",
    "            elif journal == 'poder360':\n",
    "                xpath = '/html/body/div[1]/main/div[2]/div/div[1]/article/div[1]/div[3]'\n",
    "            elif journal == 'g1':\n",
    "                xpath = '/html/body/div[2]/main/div[4]/article'\n",
    "            elif journal == 'serasa':\n",
    "                xpath = '/html/body/div[1]/div/div/main/div/div/div/div/div/div/div[2]/div[1]/div/div[2]/div/div[1]/div[5]'\n",
    "            elif journal == 'noticiasuol':\n",
    "                xpath = '/html/body/div[1]/main/article/div[1]/div[2]/div/div/div'\n",
    "            elif journal == 'em':\n",
    "                xpath = '/html/body/div[2]/div[5]/div[1]/div[1]/div/div/div/div[4]/div[1]/div/div[4]/div[2]'\n",
    "            elif journal == 'unicesumar':\n",
    "                xpath = '/html/body/div[1]/div/div[1]'\n",
    "            elif journal == 'contraponto':\n",
    "                xpath = '/html/body/div[1]/div/div/div[2]/div/div[1]/article/div[2]'\n",
    "            elif journal == 'cm7brasil':\n",
    "                xpath = '/html/body/main/div/div[2]/div/article'\n",
    "            elif journal == 'folhafinanceira':\n",
    "                xpath = '/html/body/div[2]/div/main/div/div/div[1]/div[1]/article/div[4]'\n",
    "            elif journal == 'jornalcontabil':\n",
    "                xpath = '/html/body/div[4]/div[2]/div/div/article/div/div/div[1]/div/div[1]/div/div[2]/div/div[3]/div/div[1]/div[1]'\n",
    "            else:\n",
    "                print(f\"Jornal não mapeado: {journal}\")\n",
    "                continue\n",
    "\n",
    "            text_element = WebDriverWait(driver, 3).until(\n",
    "                EC.presence_of_element_located((By.XPATH, xpath))\n",
    "            )\n",
    "            extracted_text.append(text_element.text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar texto do site {journal}: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "    return extracted_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "118a7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_requests = GetData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4481feaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GetData' object has no attribute 'from_website'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m list_data_news = \u001b[43mdo_requests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_website\u001b[49m(references_links=references_links)\n\u001b[32m      2\u001b[39m df_list_data_news = pd.DataFrame(list_data_news, columns=[\u001b[33m'\u001b[39m\u001b[33mregistros_websites\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mAttributeError\u001b[39m: 'GetData' object has no attribute 'from_website'"
     ]
    }
   ],
   "source": [
    "list_data_news = do_requests.from_website(references_links=references_links)\n",
    "df_list_data_news = pd.DataFrame(list_data_news, columns=['registros_websites'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74224d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_list_data_news.to_csv(r'../src/data/processed/df_list_data_news.csv',\n",
    "                           sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ea8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next page: t3_1l4gr4p\n",
      "Next page: t3_1l2v9ob\n",
      "Next page: t3_1l1uate\n",
      "Next page: t3_1l0395g\n",
      "Next page: t3_1kzlczg\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m list_data_reddit = \u001b[43mdo_requests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_reddit\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttps://www.reddit.com/r/golpe.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mqtd_chamadas\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m df_list_data_reddit = pd.DataFrame(list_data_reddit, columns=[\u001b[33m'\u001b[39m\u001b[33mregistros_reddit\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mGetData.from_reddit\u001b[39m\u001b[34m(self, url, qtd_chamadas)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Pega a chave selftext de cada post, por página\u001b[39;00m\n\u001b[32m     31\u001b[39m aux = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mjson_object\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mchildren\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     33\u001b[39m     aux = aux+\u001b[32m1\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(aux):\n",
      "\u001b[31mKeyError\u001b[39m: 'data'"
     ]
    }
   ],
   "source": [
    "list_data_reddit = do_requests.from_reddit(url='https://www.reddit.com/r/golpe.json',\n",
    "                                    qtd_chamadas=10)\n",
    "df_list_data_reddit = pd.DataFrame(list_data_reddit, columns=['registros_reddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e856f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_data_reddit.to_csv(r'../src/data/processed/df_list_data_reddit.csv',\n",
    "                           sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da999050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>registros_reddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O sub sempre teve como ideia ajudar e espalhar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Postei uma progressão salarial e ja veio conta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Postando novamente pq não sabia que não podia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mandei mensagem em uma pizzaria (whatsapp no s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Post anterior apagado, porque esqueci de borra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>Acho muito estranho o cara nem pedir desconto,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>Meu namorado acabou tomando um golpe tentando ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>Final de semana uma amiga foi comprar um produ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>\\n\\nEstava com minha namorada no sítio. Lugar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>Uma senhorinha que eu presto uns serviços aqui...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>526 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      registros_reddit\n",
       "0    O sub sempre teve como ideia ajudar e espalhar...\n",
       "1    Postei uma progressão salarial e ja veio conta...\n",
       "2    Postando novamente pq não sabia que não podia ...\n",
       "3    mandei mensagem em uma pizzaria (whatsapp no s...\n",
       "4    Post anterior apagado, porque esqueci de borra...\n",
       "..                                                 ...\n",
       "521  Acho muito estranho o cara nem pedir desconto,...\n",
       "522  Meu namorado acabou tomando um golpe tentando ...\n",
       "523  Final de semana uma amiga foi comprar um produ...\n",
       "524  \\n\\nEstava com minha namorada no sítio. Lugar ...\n",
       "525  Uma senhorinha que eu presto uns serviços aqui...\n",
       "\n",
       "[526 rows x 1 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list_data_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eda7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
